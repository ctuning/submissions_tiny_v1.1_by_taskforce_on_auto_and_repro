# MLPerfâ„¢ Tiny Benchmarking with MicroTVM

This is a reproducibility study performed by the [MLCommons Task Force on Automation and Reproducibility](https://github.com/mlcommons/ck/blob/master/docs/taskforce.md)
using this [original document](https://github.com/mlcommons/tiny_results_v1.0/blob/main/closed/OctoML/code/README.md).

Our goal is to make it easier for the community to run all MLPerf benchmarks out of the box on any software and hardware from any vendor,
automate optimization experiments, reproduce results, and synthesize Pareto-optimal end-to-end applications
with the help of the [MLCommons CM automation language](https://github.com/mlcommons/ck) and the [MLCommons CK playground](https://x.cKnowledge.org).
